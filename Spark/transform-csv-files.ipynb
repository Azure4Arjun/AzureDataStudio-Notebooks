{
    "metadata": {
        "kernelspec": {
            "name": "pysparkkernel",
            "display_name": "PySpark",
            "language": ""
        },
        "language_info": {
            "name": "pyspark",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Spark sample showing read/write methods\n",
                "In this sample notebook, we will read CSV file(s) from HDFS, write it as parquet & orc file(s) and save a Hive table definition."
            ],
            "metadata": {
                "azdata_cell_guid": "411b016a-bde1-4095-9708-6f27f1e7f7c3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Read the clickstream CSV file(s) into a spark data frame, print schema & top rows\n",
                "results = spark.read.option(\"inferSchema\", \"true\").csv('/clickstream_data').toDF(\n",
                "            \"wcs_click_date_sk\", \"wcs_click_time_sk\", \"wcs_sales_sk\", \"wcs_item_sk\", \"wcs_web_page_sk\", \"wcs_user_sk\"\n",
                "            )\n",
                "results.printSchema()\n",
                "results.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "245a0a5d-0a6f-496b-81bd-6a11663a7cdc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Disable saving SUCCESS file\r\n",
                "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \r\n",
                "\r\n",
                "# Print the current warehouse directory where the parquet files will be stored\r\n",
                "print(spark.conf.get(\"spark.sql.warehouse.dir\"))\r\n",
                "\r\n",
                "# Save results as parquet & orc file and create hive table\r\n",
                "results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"web_clickstreams\")\r\n",
                "results.write.format(\"orc\").mode(\"overwrite\").saveAsTable(\"web_clickstreams_orc\")"
            ],
            "metadata": {
                "azdata_cell_guid": "157e8d90-8c2a-49e7-8dfd-7a80d69d5bb1"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\n",
                "results = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\r\n",
                "            \"pr_review_sk\", \"pr_review_content\"\r\n",
                "            )\r\n",
                "results.printSchema()\r\n",
                "results.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "9621447f-43c1-4ff4-9232-42f89863af75"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Save results as parquet, and orc formats and create hive table\r\n",
                "results.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"product_reviews\")\r\n",
                "results.write.format(\"orc\").mode(\"overwrite\").saveAsTable(\"product_reviews_orc\")"
            ],
            "metadata": {
                "azdata_cell_guid": "d9994456-9b2f-4951-902d-119af9f61906"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}